{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Feature importances with a forest of trees\n\nThis example shows the use of a forest of trees to evaluate the importance of\nfeatures on an artificial classification task. The blue bars are the feature\nimportances of the forest, along with their inter-trees variability represented\nby the error bars.\n\nAs expected, the plot suggests that 3 features are informative, while the\nremaining are not.\n\nThe base code has been adapted from the\n[original scikit-learn example](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-download-auto-examples-ensemble-plot-forest-importances-py)\n\nTo learn about the benefits of permuted performance over the importance captured when a model is trained you should\nrefer to that original example.  This example will focus on the interactive feature importance plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nimport plotly\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom elphick.sklearn_viz.features import plot_feature_importance, FeatureImportance\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG,\n                    format='%(asctime)s %(levelname)s %(module)s - %(funcName)s: %(message)s',\n                    datefmt='%Y-%m-%dT%H:%M:%S%z')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data generation and model fitting\nWe generate a synthetic dataset with only 3 informative features. We will\nexplicitly not shuffle the dataset to ensure that the informative features\nwill correspond to the three first columns of X.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = make_classification(\n    n_samples=1000,\n    n_features=10,\n    n_informative=3,\n    n_redundant=0,\n    n_repeated=0,\n    n_classes=2,\n    random_state=0,\n    shuffle=False,\n)\nX = pd.DataFrame(X, columns=[f\"Feature {i}\" for i in range(1, X.shape[1]+1)])\ny = pd.Series(y, name='Class')\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A random forest classifier will be fitted to compute the feature importances.\n\n.. note ::\n    To obtain the real feature names in the plot the following is needed:\n\n    - Pass pd.DataFrames to the fit method\n    - Set the transform output to \"pandas\"\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe = make_pipeline(SelectKBest(k='all'), RandomForestClassifier(random_state=0))\npipe.set_output(transform=\"pandas\")\npipe.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature importance based on mean decrease in impurity\nLong story short, this approach is faster, since it comes as an output of model fitting, but is less accurate.\n\nCreate an interactive Feature Importance plot\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot_feature_importance(pipe)\nfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plot_feature_importance(pipe, sort=True, top_k=5)\nfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = FeatureImportance(pipe).plot(horizontal=True, sort=True, top_k=5)\nfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_importance: pd.DataFrame = FeatureImportance(pipe).data\nfeature_importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the three first features are found important.\n\n## Feature importance based on feature permutation\nThis approach takes longer but is better.\n\nCreate an interactive Feature Importance plot using permutation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fi = FeatureImportance(pipe, permute=True, x_test=X_test, y_test=y_test)\nfig = fi.plot()\nfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = fi.plot(horizontal=True, sort=True, top_k=5)\n# noinspection PyTypeChecker\nplotly.io.show(fig)  # this call to show will set the thumbnail for use in the gallery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same features are detected as most important using both methods. Although\nthe relative importances vary. As seen on the plots, MDI is less likely than\npermutation importance to fully omit a feature.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If features are engineered, the reported feature importances (by default) will include the engineered features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe2 = make_pipeline(PolynomialFeatures(degree=2), RandomForestClassifier(random_state=0))\npipe2.set_output(transform=\"pandas\")\npipe2.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fi = FeatureImportance(pipe2, permute=True, x_test=X_test, y_test=y_test)\nfig = fi.plot(sort=True, top_k=10)\nfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this we set the pipeline_input_features parameter to True.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fi = FeatureImportance(pipe2, permute=True, x_test=X_test, y_test=y_test, pipeline_input_features=True)\nfig = fi.plot()\nfig"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}